{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "4AkCDjc_j_N3"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import model_selection\n",
    "import re\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "Cwe49JKKq4RG"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>Answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>hi, how are you doing?</td>\n",
       "      <td>i'm fine. how about yourself?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>i'm fine. how about yourself?</td>\n",
       "      <td>i'm pretty good. thanks for asking.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>i'm pretty good. thanks for asking.</td>\n",
       "      <td>no problem. so how have you been?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>no problem. so how have you been?</td>\n",
       "      <td>i've been great. what about you?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>i've been great. what about you?</td>\n",
       "      <td>i've been good. i'm in school right now.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4286</th>\n",
       "      <td>The Hubble Space Telescope, launched into low ...</td>\n",
       "      <td>Edwin Hubble</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4287</th>\n",
       "      <td>What is the name of the nearest major galaxy t...</td>\n",
       "      <td>The Andromeda Galaxy.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4288</th>\n",
       "      <td>God Save the Queen is the national anthem of w...</td>\n",
       "      <td>The United Kingdom of Great Britain</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4289</th>\n",
       "      <td>The Celtic Shelf, the seabed under the Celtic ...</td>\n",
       "      <td>Europe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4290</th>\n",
       "      <td>Dolphins use a sense, similar to sonar, to det...</td>\n",
       "      <td>Echolocation</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4291 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               question  \\\n",
       "0                                hi, how are you doing?   \n",
       "1                         i'm fine. how about yourself?   \n",
       "2                   i'm pretty good. thanks for asking.   \n",
       "3                     no problem. so how have you been?   \n",
       "4                      i've been great. what about you?   \n",
       "...                                                 ...   \n",
       "4286  The Hubble Space Telescope, launched into low ...   \n",
       "4287  What is the name of the nearest major galaxy t...   \n",
       "4288  God Save the Queen is the national anthem of w...   \n",
       "4289  The Celtic Shelf, the seabed under the Celtic ...   \n",
       "4290  Dolphins use a sense, similar to sonar, to det...   \n",
       "\n",
       "                                        Answer  \n",
       "0                i'm fine. how about yourself?  \n",
       "1          i'm pretty good. thanks for asking.  \n",
       "2            no problem. so how have you been?  \n",
       "3             i've been great. what about you?  \n",
       "4     i've been good. i'm in school right now.  \n",
       "...                                        ...  \n",
       "4286                              Edwin Hubble  \n",
       "4287                     The Andromeda Galaxy.  \n",
       "4288       The United Kingdom of Great Britain  \n",
       "4289                                    Europe  \n",
       "4290                              Echolocation  \n",
       "\n",
       "[4291 rows x 2 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#read dataset\n",
    "data_df = pd.read_csv('chatbot.csv', index_col=False)\n",
    "data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LVCMY5yA-meq"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4t0fyUNI-rak",
    "outputId": "db52bac7-03c3-43de-d6a1-c78f44c38655"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 4291 entries, 0 to 4290\n",
      "Data columns (total 2 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   question  4291 non-null   object\n",
      " 1   Answer    4291 non-null   object\n",
      "dtypes: object(2)\n",
      "memory usage: 67.2+ KB\n"
     ]
    }
   ],
   "source": [
    "#details of data\n",
    "data_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "mS8P5lh7-wpW"
   },
   "outputs": [],
   "source": [
    "data_df.columns=[\"question\",\"answer\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "bkZ_LazP-1eL"
   },
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "  text = text.lower()\n",
    "  text = re.sub('\\[.*?\\]', '', text)\n",
    "  text = re.sub('https?://\\S+|www\\.\\S+', '', text)\n",
    "  text = re.sub('<.*?>+', '', text)\n",
    "  text = re.sub('\\n', '', text)\n",
    "  text = re.sub(r'[^\\w]',' ',text)\n",
    "  text = re.sub('\\w*\\d\\w*', '', text)\n",
    "  return text\n",
    "\n",
    "data_df.question = data_df.question.map(clean_text)\n",
    "data_df.answer = data_df.answer.map(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "7kdgb-Gw-5Lf"
   },
   "outputs": [],
   "source": [
    "def add_start_end(text):\n",
    "  text = f'<start> {text} <end>'\n",
    "  return text\n",
    "\n",
    "data_df.question = data_df.question.map(add_start_end)\n",
    "data_df.answer = data_df.answer.map(add_start_end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "id": "YrEEiyqO_L-y",
    "outputId": "e14d5a42-85c3-4df4-d393-239e40b9e7f8"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;start&gt; hi  how are you doing  &lt;end&gt;</td>\n",
       "      <td>&lt;start&gt; i m fine  how about yourself  &lt;end&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>&lt;start&gt; i m fine  how about yourself  &lt;end&gt;</td>\n",
       "      <td>&lt;start&gt; i m pretty good  thanks for asking  &lt;end&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>&lt;start&gt; i m pretty good  thanks for asking  &lt;end&gt;</td>\n",
       "      <td>&lt;start&gt; no problem  so how have you been  &lt;end&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>&lt;start&gt; no problem  so how have you been  &lt;end&gt;</td>\n",
       "      <td>&lt;start&gt; i ve been great  what about you  &lt;end&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>&lt;start&gt; i ve been great  what about you  &lt;end&gt;</td>\n",
       "      <td>&lt;start&gt; i ve been good  i m in school right no...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4286</th>\n",
       "      <td>&lt;start&gt; the hubble space telescope  launched i...</td>\n",
       "      <td>&lt;start&gt; edwin hubble &lt;end&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4287</th>\n",
       "      <td>&lt;start&gt; what is the name of the nearest major ...</td>\n",
       "      <td>&lt;start&gt; the andromeda galaxy  &lt;end&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4288</th>\n",
       "      <td>&lt;start&gt; god save the queen is the national ant...</td>\n",
       "      <td>&lt;start&gt; the united kingdom of great britain &lt;end&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4289</th>\n",
       "      <td>&lt;start&gt; the celtic shelf  the seabed under the...</td>\n",
       "      <td>&lt;start&gt; europe &lt;end&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4290</th>\n",
       "      <td>&lt;start&gt; dolphins use a sense  similar to sonar...</td>\n",
       "      <td>&lt;start&gt; echolocation &lt;end&gt;</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4291 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               question  \\\n",
       "0                  <start> hi  how are you doing  <end>   \n",
       "1           <start> i m fine  how about yourself  <end>   \n",
       "2     <start> i m pretty good  thanks for asking  <end>   \n",
       "3       <start> no problem  so how have you been  <end>   \n",
       "4        <start> i ve been great  what about you  <end>   \n",
       "...                                                 ...   \n",
       "4286  <start> the hubble space telescope  launched i...   \n",
       "4287  <start> what is the name of the nearest major ...   \n",
       "4288  <start> god save the queen is the national ant...   \n",
       "4289  <start> the celtic shelf  the seabed under the...   \n",
       "4290  <start> dolphins use a sense  similar to sonar...   \n",
       "\n",
       "                                                 answer  \n",
       "0           <start> i m fine  how about yourself  <end>  \n",
       "1     <start> i m pretty good  thanks for asking  <end>  \n",
       "2       <start> no problem  so how have you been  <end>  \n",
       "3        <start> i ve been great  what about you  <end>  \n",
       "4     <start> i ve been good  i m in school right no...  \n",
       "...                                                 ...  \n",
       "4286                         <start> edwin hubble <end>  \n",
       "4287                <start> the andromeda galaxy  <end>  \n",
       "4288  <start> the united kingdom of great britain <end>  \n",
       "4289                               <start> europe <end>  \n",
       "4290                         <start> echolocation <end>  \n",
       "\n",
       "[4291 rows x 2 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "HqcEA2Ca_AGR"
   },
   "outputs": [],
   "source": [
    "#nlp process\n",
    "# fea ture ext\n",
    "# lemmatizing\n",
    "# \n",
    "def tokenize(lang):\n",
    "  lang_tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
    "      filters='!\"#$%&()*+,-./:;=?@[\\\\]^_`{|}~\\t\\n', oov_token='<OOV>'\n",
    "  )\n",
    "  lang_tokenizer.fit_on_texts(lang)\n",
    "  tensor = lang_tokenizer.texts_to_sequences(lang)\n",
    "  tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding='post')\n",
    "  return tensor, lang_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "FyLWJLub_T2g"
   },
   "outputs": [],
   "source": [
    "\n",
    "question_sequence, question_tokenizer = tokenize(data_df.question)\n",
    "answer_sequence, answer_tokenizer = tokenize(data_df.answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "57LUNTez_Xqf"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((3861, 24), (430, 24), (3861, 74), (430, 74))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#feature selection\n",
    "x_train, x_test, y_train, y_test = model_selection.train_test_split(question_sequence, \n",
    "                answer_sequence, test_size = 0.1, random_state=42) \n",
    "\n",
    "x_train.shape, x_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "EdEAhSLm_e0-"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question\n",
      "2---> <start>\n",
      "60---> tell\n",
      "20---> me\n",
      "7---> a\n",
      "95---> joke\n",
      "3---> <end>\n",
      "\n",
      "Answer\n",
      "2---> <start>\n",
      "14---> what\n",
      "16---> do\n",
      "5---> you\n",
      "42---> get\n",
      "64---> when\n",
      "5---> you\n",
      "139---> cross\n",
      "356---> music\n",
      "15---> and\n",
      "72---> an\n",
      "1723---> automobile\n",
      "3021---> cartune\n",
      "3---> <end>\n"
     ]
    }
   ],
   "source": [
    "def convert(lang, tensor):\n",
    "  for t in tensor:\n",
    "    if t!=0:\n",
    "      print('%d---> %s' % (t, lang.index_word[t]))\n",
    "\n",
    "print('Question')\n",
    "convert(question_tokenizer, x_train[0])\n",
    "print()\n",
    "print('Answer')\n",
    "convert(answer_tokenizer, y_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "UqozKhHJABV4"
   },
   "outputs": [],
   "source": [
    "#length of sentence\n",
    "vocab_inp_size = len(question_tokenizer.word_index)+1\n",
    "vocab_tar_size =  len(answer_tokenizer.word_index)+1\n",
    "#number of hidden layers\n",
    "embedding_dim = 256\n",
    "#number of neuron\n",
    "units = 1024\n",
    "batch_size=32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "0XHxm5XyAJ4S"
   },
   "outputs": [],
   "source": [
    "#create train_dataset,test_dataset\n",
    "def create_dataset(x, y, batch_size=32):\n",
    "  data = tf.data.Dataset.from_tensor_slices((x, y))\n",
    "\n",
    "  data = data.shuffle(1028)\n",
    "  data = data.batch(batch_size, drop_remainder=True)\n",
    "\n",
    "  data = data.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "  return data\n",
    "\n",
    "train_dataset = create_dataset(x_train, y_train)\n",
    "test_dataset = create_dataset(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "U9XR7Em8AT7G"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question:(32, 24)\n",
      "[[   2   22  102 2233    3    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0]\n",
      " [   2    6  188  786    3    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0]\n",
      " [   2    5   71    7 1142  112   91    3    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0]\n",
      " [   2    6  715   25   49   47    3    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0]\n",
      " [   2   13   12   78   72 1567   80 1568    3    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0]\n",
      " [   2    9 1969 1970    3    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0]\n",
      " [   2  106   17    9   13  108    4  246    3    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0]\n",
      " [   2    7  332    4  667   58    6  260    3    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0]\n",
      " [   2   27   93   29  134  381   58    3    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0]\n",
      " [   2   15    5   54  372    3    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0]\n",
      " [   2   11   16   36  203 2366    3    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0]\n",
      " [   2   29   32   14  173   50   11  190  251   17  115   16    3    0\n",
      "     0    0    0    0    0    0    0    0    0    0]\n",
      " [   2   12   10   69   38   60   20   11   65  301   24    3    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0]\n",
      " [   2   18    5  164  120  115    3    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0]\n",
      " [   2  400    3    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0]\n",
      " [   2   60   20   39   59  726    3    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0]\n",
      " [   2 1688  147   14   15  130   49    3    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0]\n",
      " [   2   12   10    7   49  511    3    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0]\n",
      " [   2    4   32   14   52    8   43  835    3    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0]\n",
      " [   2    9   10  178   49    8   18  379    3    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0]\n",
      " [   2  113 2092  395 1372   17  396  761    3    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0]\n",
      " [   2   38    6  194  463   25  226  310   17  552   33  602    3    0\n",
      "     0    0    0    0    0    0    0    0    0    0]\n",
      " [   2   38    5   64  154   85 1289   68   65   25  157    3    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0]\n",
      " [   2    5   16 1478    3    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0]\n",
      " [   2    4   56   24   59  142    3    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0]\n",
      " [   2   61  171  821    9  165    6  142    3    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0]\n",
      " [   2    4   32   14   18    7  377  675    3    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0]\n",
      " [   2    4   92    9  120  816   10    3    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0]\n",
      " [   2    9   10 1904   42    7  359  390   39    7  461  272   19  128\n",
      "   716  881    3    0    0    0    0    0    0    0]\n",
      " [   2    4   52    8  439    8   88  993    3    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0]\n",
      " [   2    4   30  252   33    5    3    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0]\n",
      " [   2   16    5   48    8  287    7  345    3    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0]]\n",
      "Answer:(32, 74)\n",
      "[[  2 172 120 ...   0   0   0]\n",
      " [  2   9 337 ...   0   0   0]\n",
      " [  2  46   3 ...   0   0   0]\n",
      " ...\n",
      " [  2   8   6 ...   0   0   0]\n",
      " [  2 141   5 ...   0   0   0]\n",
      " [  2  36  34 ...   0   0   0]]\n"
     ]
    }
   ],
   "source": [
    "for q, a in train_dataset.take(1):\n",
    "  print(f'Question:{q.shape}\\n{q}')\n",
    "  \n",
    "  print(f'Answer:{a.shape}\\n{a}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "cMcLSqk7AXlr"
   },
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.Model):\n",
    "  def __init__(self, vocab_size, embedding_dim, encoder_units, batch_size):\n",
    "      super(Encoder, self).__init__()\n",
    "\n",
    "      self.batch_size = batch_size\n",
    "      self.encoder_units = encoder_units\n",
    "      self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim, mask_zero=True)\n",
    "      self.gru = tf.keras.layers.GRU(self.encoder_units, \n",
    "                                           return_sequences=True,\n",
    "                                           return_state=True,\n",
    "                                           recurrent_initializer = 'glorot_uniform')\n",
    "\n",
    "  def call(self, x, hidden):\n",
    "    x = self.embedding(x)\n",
    "    output, state = self.gru(x, initial_state = hidden)\n",
    "    return output, state\n",
    "\n",
    "  def initialize_hidden_state(self):\n",
    "    return tf.zeros((self.batch_size, self.encoder_units))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "lzl058FSAdy4"
   },
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.Model):\n",
    "  def __init__(self, vocab_size, embedding_dim, decoder_units, batch_size):\n",
    "      super(Decoder, self).__init__()\n",
    "\n",
    "      self.batch_size = batch_size\n",
    "      self.decoder_units = decoder_units\n",
    "      self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim, mask_zero=True)\n",
    "      self.gru = tf.keras.layers.GRU(self.decoder_units, \n",
    "                                           return_sequences=True,\n",
    "                                           return_state=True,\n",
    "                                           recurrent_initializer = 'glorot_uniform')\n",
    "      \n",
    "      self.fc = tf.keras.layers.Dense(vocab_size)\n",
    "\n",
    "\n",
    "  def call(self, x, hidden):\n",
    "    x = self.embedding(x)\n",
    "    output, hidden = self.gru(x, initial_state = hidden)\n",
    "    output = tf.reshape(output, (-1, output.shape[2]))\n",
    "    x =  tf.nn.softmax(self.fc(output))\n",
    "    return x, hidden\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c0N05vE3Aj2m",
    "outputId": "328e20bd-fa08-4b61-c37b-72220852d9e9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder output shape: (batch size, sequence length, units) (32, 24, 1024)\n",
      "Encoder Hidden state shape: (batch size, units) (32, 1024)\n"
     ]
    }
   ],
   "source": [
    "encoder = Encoder(vocab_inp_size, embedding_dim, units, batch_size)\n",
    "sample_hidden = encoder.initialize_hidden_state()\n",
    "sample_output, sample_hidden = encoder(q, sample_hidden)\n",
    "print ('Encoder output shape: (batch size, sequence length, units) {}'.format(sample_output.shape))\n",
    "print ('Encoder Hidden state shape: (batch size, units) {}'.format(sample_hidden.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MB0BbIzDAsb-",
    "outputId": "295068c0-d748-4c0d-ab63-c43d194509fe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoder output shape: (batch size, vocab_size) (32, 3319)\n"
     ]
    }
   ],
   "source": [
    "decoder = Decoder(vocab_tar_size, embedding_dim, units, batch_size)\n",
    "\n",
    "sample_decoder_output, _ = decoder(tf.random.uniform((batch_size, 1)), sample_hidden)\n",
    "\n",
    "print ('Decoder output shape: (batch size, vocab_size) {}'.format(sample_decoder_output.shape))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "LWFmNtmvAw_f"
   },
   "outputs": [],
   "source": [
    "# create the optimizer using the Adam optimizer\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "# create the loss function\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=False, reduction='none')\n",
    "\n",
    "# define the loss function for the training\n",
    "def loss_function(real, pred):\n",
    "  # create the mask to ignore the padding tokens\n",
    "  mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "  # mask shape == (batch_size, sequence_length)\n",
    "  # calculate the loss\n",
    "  loss_ = loss_object(real, pred)\n",
    "  # mask the loss\n",
    "  # how the mask works:\n",
    "  # if the value is 1, the loss is calculated\n",
    "  # if the value is 0, the loss is ignored\n",
    "    #[1,1,1,1,1,1,0,0,0,0,0] mask\n",
    "    # *\n",
    "    #[2,6,2,1,6,3,2,1,5,7,9] input\n",
    "    # =\n",
    "    #[2,6,2,1,6,3,0,0,0,0,0] output\n",
    "  mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "  # mask shape == (batch_size, sequence_length)\n",
    "\n",
    "  loss_ *= mask\n",
    "  # calculate the average loss per batch \n",
    "  return tf.reduce_mean(loss_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "vckGQ7FUA5kH"
   },
   "outputs": [],
   "source": [
    "# create the training metric \n",
    "train_loss = tf.metrics.Mean(name='train loss')\n",
    "# create the testing metric \n",
    "test_loss =tf.metrics.Mean(name='test loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "R-etVcTQA-AG"
   },
   "outputs": [],
   "source": [
    "# create the training step\n",
    "# using the tf.function decorator to speed up the training process by converting the training function to a TensorFlow graph\n",
    "@tf.function\n",
    "# define the training step \n",
    "def train_step(inputs, target, enc_hidden):\n",
    "  # the encoder_hidden is the initial hidden state of the encoder\n",
    "  # enc_hidden shape == (batch_size, hidden_size)\n",
    "\n",
    "  # inilaize the loss to zero\n",
    "  loss = 0\n",
    "  # create the gradient tape to record the gradient of the loss with respect to the weights\n",
    "\n",
    "  with tf.GradientTape() as tape:\n",
    "    # pass the input to the encoder\n",
    "    # enc_output shape == (batch_size, 49, hidden_size)\n",
    "    # enc_hidden shape == (batch_size, hidden_size)\n",
    "    # using the encoder to get the encoder_output and the encoder_hidden\n",
    "    # using the encoder_hidden as the initial hidden state of the decoder\n",
    "    enc_output, enc_hidden = encoder(inputs, enc_hidden)\n",
    "    # set the initial decoder hidden state to the encoder hidden state\n",
    "    dec_hidden = enc_hidden\n",
    "\n",
    "    # create the start token \n",
    "    # start_token shape == (batch_size, 1)\n",
    "    # repeat the start token for the batch size times\n",
    "    dec_input = tf.expand_dims([answer_tokenizer.word_index['<start>']] * inputs.shape[0], 1)\n",
    "    \n",
    "    # Teacher forcing - feeding the target as the next input\n",
    "    \n",
    "    for t in range(1, target.shape[1]):\n",
    "      # passing enc_output to the decoder\n",
    "      predictions, dec_hidden = decoder(dec_input, dec_hidden)\n",
    "      # calculate the loss for the current time step using the loss function\n",
    "      loss += loss_function(target[:, t], predictions)\n",
    "\n",
    "      # using teacher forcing\n",
    "      dec_input = tf.expand_dims(target[:, t], 1)\n",
    "  # calculate the loss for the current batch\n",
    "  batch_loss = (loss / int(target.shape[1]))\n",
    "\n",
    "  # get the trainable variables\n",
    "  variables = encoder.trainable_variables + decoder.trainable_variables\n",
    "  # calculate the gradients using the tape \n",
    "  gradients = tape.gradient(loss, variables)\n",
    "  # update the trainable variables\n",
    "  optimizer.apply_gradients(zip(gradients, variables))\n",
    "  # add the loss to the training loss metric\n",
    "  train_loss(batch_loss)\n",
    "  return batch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "4MkhVyXgBGuD"
   },
   "outputs": [],
   "source": [
    "# create the training step\n",
    "# using the tf.function decorator to speed up the training process by converting the training function to a TensorFlow graph\n",
    "@tf.function \n",
    "def test_step(inputs, target, enc_hidden):\n",
    "    # the encoder_hidden is the initial hidden state of the encoder\n",
    "    # enc_hidden shape == (batch_size, hidden_size)\n",
    "    # inilaize the loss to zero\n",
    "    loss = 0\n",
    "    # pass the input to the encoder \n",
    "    # enc_output shape == (batch_size, 49, hidden_size) \n",
    "    # enc_hidden shape == (batch_size, hidden_size)\n",
    "    # using the encoder to get the encoder_output and the encoder_hidden\n",
    "    enc_output, enc_hidden = encoder(inputs, enc_hidden)\n",
    "    # set the initial decoder hidden state to the encoder hidden state\n",
    "    dec_hidden = enc_hidden\n",
    "    # create the start token\n",
    "    # start_token shape == (batch_size, 1)\n",
    "    # repeat the start token for the batch size times\n",
    "    dec_input = tf.expand_dims([answer_tokenizer.word_index['<start>']] * inputs.shape[0], 1)\n",
    "    for t in range(1, target.shape[1]):\n",
    "        # passing enc_output to the decoder with dec_hidden as the initial hidden state\n",
    "        predictions, dec_hidden = decoder(dec_input, dec_hidden)\n",
    "        # calculate the loss for the current time step using the loss function \n",
    "        loss += loss_function(target[:, t], predictions)\n",
    "\n",
    "        # using teacher forcing\n",
    "        dec_input = tf.expand_dims(target[:, t], 1)\n",
    "    # calculate the loss for the current batch\n",
    "    batch_loss = (loss / int(target.shape[1]))\n",
    "    # add the batch loss to the test loss metric\n",
    "    test_loss(batch_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "5vfZHFDFBkIo"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "133/134 [============================>.] - ETA: 3sModel is saved\n",
      "##################################################\n",
      "Epoch #1\n",
      "Accuracy 0.5891615152359009\n",
      "##################################################\n",
      "133/134 [============================>.] - ETA: 4:18Model is saved\n",
      "##################################################\n",
      "Epoch #2\n",
      "Accuracy 0.5588918328285217\n",
      "##################################################\n",
      "133/134 [============================>.] - ETA: 3sModel is saved\n",
      "##################################################\n",
      "Epoch #3\n",
      "Accuracy 0.55002361536026\n",
      "##################################################\n",
      "133/134 [============================>.] - ETA: 3s##################################################\n",
      "Epoch #4\n",
      "Accuracy 0.5584268569946289\n",
      "##################################################\n",
      "133/134 [============================>.] - ETA: 3s##################################################\n",
      "Epoch #5\n",
      "Accuracy 0.5645029544830322\n",
      "##################################################\n",
      "133/134 [============================>.] - ETA: 3s##################################################\n",
      "Epoch #6\n",
      "Accuracy 0.5821784138679504\n",
      "##################################################\n",
      "133/134 [============================>.] - ETA: 3s##################################################\n",
      "Epoch #7\n",
      "Accuracy 0.5971917510032654\n",
      "##################################################\n",
      "133/134 [============================>.] - ETA: 3s##################################################\n",
      "Epoch #8\n",
      "Accuracy 0.5968561768531799\n",
      "##################################################\n",
      "133/134 [============================>.] - ETA: 3s##################################################\n",
      "Epoch #9\n",
      "Accuracy 0.6266809105873108\n",
      "##################################################\n",
      "133/134 [============================>.] - ETA: 3s##################################################\n",
      "Epoch #10\n",
      "Accuracy 0.6400573253631592\n",
      "##################################################\n",
      "133/134 [============================>.] - ETA: 3s##################################################\n",
      "Epoch #11\n",
      "Accuracy 0.6476496458053589\n",
      "##################################################\n",
      "133/134 [============================>.] - ETA: 3s##################################################\n",
      "Epoch #12\n",
      "Accuracy 0.6687183380126953\n",
      "##################################################\n",
      "133/134 [============================>.] - ETA: 3s##################################################\n",
      "Epoch #13\n",
      "Accuracy 0.672971785068512\n",
      "##################################################\n",
      "133/134 [============================>.] - ETA: 3s##################################################\n",
      "Epoch #14\n",
      "Accuracy 0.6979113221168518\n",
      "##################################################\n",
      "133/134 [============================>.] - ETA: 3s##################################################\n",
      "Epoch #15\n",
      "Accuracy 0.701331377029419\n",
      "##################################################\n",
      "133/134 [============================>.] - ETA: 3s##################################################\n",
      "Epoch #16\n",
      "Accuracy 0.702736496925354\n",
      "##################################################\n",
      "133/134 [============================>.] - ETA: 3s##################################################\n",
      "Epoch #17\n",
      "Accuracy 0.7052732706069946\n",
      "##################################################\n",
      "133/134 [============================>.] - ETA: 3s##################################################\n",
      "Epoch #18\n",
      "Accuracy 0.74072265625\n",
      "##################################################\n",
      "133/134 [============================>.] - ETA: 3s##################################################\n",
      "Epoch #19\n",
      "Accuracy 0.7307215332984924\n",
      "##################################################\n",
      "133/134 [============================>.] - ETA: 3s##################################################\n",
      "Epoch #20\n",
      "Accuracy 0.7577148079872131\n",
      "##################################################\n",
      "133/134 [============================>.] - ETA: 3s##################################################\n",
      "Epoch #21\n",
      "Accuracy 0.7643918395042419\n",
      "##################################################\n",
      "133/134 [============================>.] - ETA: 3s##################################################\n",
      "Epoch #22\n",
      "Accuracy 0.7802218794822693\n",
      "##################################################\n",
      "133/134 [============================>.] - ETA: 3s##################################################\n",
      "Epoch #23\n",
      "Accuracy 0.8006867170333862\n",
      "##################################################\n",
      "133/134 [============================>.] - ETA: 3s##################################################\n",
      "Epoch #24\n",
      "Accuracy 0.8016999959945679\n",
      "##################################################\n",
      "133/134 [============================>.] - ETA: 3s##################################################\n",
      "Epoch #25\n",
      "Accuracy 0.8151907324790955\n",
      "##################################################\n",
      "133/134 [============================>.] - ETA: 3s##################################################\n",
      "Epoch #26\n",
      "Accuracy 0.820659875869751\n",
      "##################################################\n",
      "133/134 [============================>.] - ETA: 3s##################################################\n",
      "Epoch #27\n",
      "Accuracy 0.823810875415802\n",
      "##################################################\n",
      "133/134 [============================>.] - ETA: 3s##################################################\n",
      "Epoch #28\n",
      "Accuracy 0.8023747801780701\n",
      "##################################################\n",
      "133/134 [============================>.] - ETA: 3s##################################################\n",
      "Epoch #29\n",
      "Accuracy 0.8333476781845093\n",
      "##################################################\n",
      "133/134 [============================>.] - ETA: 3s##################################################\n",
      "Epoch #30\n",
      "Accuracy 0.8293602466583252\n",
      "##################################################\n"
     ]
    }
   ],
   "source": [
    "# set the epochs to 10\n",
    "EPOCHS = 30\n",
    "# set the old test loss to high number \n",
    "\n",
    "old_test_loss=1000000\n",
    "# create the training loop\n",
    "for epoch in range(EPOCHS):\n",
    "    # reset the training loss metric\n",
    "    train_loss.reset_states()\n",
    "    # reset the testing loss metric\n",
    "    test_loss.reset_states()\n",
    "\n",
    "    # initalize the hidden state of the encoder to zeros \n",
    "    enc_hidden = encoder.initialize_hidden_state()\n",
    "    # create the training progress bar set the total number of batches to the length of the training dataset and the batch size to the test size\n",
    "    steps_per_epoch = answer_sequence.shape[0]//batch_size #=> 4356 batch in the dataset \n",
    "    bar = tf.keras.utils.Progbar(target=steps_per_epoch)\n",
    "    \n",
    "    count=0\n",
    "    # iterate over the training dataset \n",
    "    for (batch, (inputs, target)) in enumerate(train_dataset):\n",
    "        # update the progress bar\n",
    "        count += 1\n",
    "        # run the training step\n",
    "        batch_loss = train_step(inputs, target, enc_hidden)\n",
    "        bar.update(count)  # manually update the progress bar    \n",
    "    # iterate over the testing dataset    \n",
    "    for (batch, (inputs, target)) in enumerate(test_dataset):\n",
    "        count += 1\n",
    "        # run the testing step\n",
    "        batch_loss = test_step(inputs, target, enc_hidden)\n",
    "        bar.update(count)\n",
    "    # save the best performance model on the test dataset \n",
    "    \n",
    "    if old_test_loss> test_loss.result():\n",
    "        # set the old test loss to the test loss \n",
    "        old_test_loss= test_loss.result()\n",
    "        encoder.save_weights(filepath='/content/models/encoder')\n",
    "        decoder.save_weights(filepath='/content/models/decoder')\n",
    "        print('Model is saved')\n",
    "    # print the training and testing loss\n",
    "    print('#' * 50)\n",
    "    print(f'Epoch #{epoch + 1}')\n",
    "    print(f'Accuracy {test_loss.result()}')\n",
    "    print('#' * 50)\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "WaTmJsVdBwLl"
   },
   "outputs": [],
   "source": [
    "# create the chatbot function\n",
    "# the chatbot function takes in the question as input and answers the input sentence \n",
    "def chatbot(sentence):\n",
    "  \n",
    "  # clean the input question sentence \n",
    "  sentence = clean_text(sentence)\n",
    "  # add the start token to the sentence\n",
    "  sentence =add_start_end(sentence)\n",
    "  # tokenize the sentence\n",
    "  inputs = question_tokenizer.texts_to_sequences([sentence])\n",
    "  # pad the sentence\n",
    "  inputs = tf.keras.preprocessing.sequence.pad_sequences(inputs,\n",
    "                                                         maxlen=29,\n",
    "                                                         padding='post')\n",
    "  \n",
    "  # initalize the hidden state of the encoder to zeros\n",
    "  hidden = [tf.zeros((1, units))]\n",
    "  # pass the sentence to the encoder with the hidden state as the initial hidden state\n",
    "  enc_out, enc_hidden = encoder(inputs, hidden)\n",
    "  # set the initial decoder hidden state to the encoder hidden state\n",
    "  dec_hidden = enc_hidden\n",
    "  # create the start token\n",
    "  # start_token shape == (batch_size, 1)\n",
    "  # repeat the start token for the batch size times\n",
    "  dec_input = tf.expand_dims([answer_tokenizer.word_index['<start>']], 0)\n",
    "  # create the result string\n",
    "  result = ''\n",
    "  # loop over the length of the sentence (32)\n",
    "\n",
    "  for t in range(32):\n",
    "    # passing the encoder output and the decoder hidden state to the decoder make sure the decoder input is the previous predicted word\n",
    "    predictions, dec_hidden = decoder(dec_input, dec_hidden)\n",
    "\n",
    "    # getting the predicted word index\n",
    "    predicted_id = tf.argmax(predictions[0]).numpy()\n",
    "    # getting the predicted word using the predicted index\n",
    "    # add the predicted word to the result string \n",
    "    result += answer_tokenizer.index_word[predicted_id] + ' '\n",
    "    # if the predicted word is the <end> token then stop the loop\n",
    "    if answer_tokenizer.index_word[predicted_id] == '<end>':\n",
    "      # remove the <start> and <end> tokens from the result string\n",
    "      result = result.replace('<start> ', '')\n",
    "      result = result.replace(' <end> ','')\n",
    "      # remove the <start> and <end> tokens from the sentence string\n",
    "      sentence = sentence.replace('<start> ', '')\n",
    "      sentence = sentence.replace(' <end>', '')\n",
    "      return  sentence, result\n",
    "\n",
    "    # using the predicted word as the next decoder input\n",
    "    dec_input = tf.expand_dims([predicted_id], 0)\n",
    "  # remove the <start> and <end> tokens from the result string\n",
    "  result = result.replace('<start> ', '')\n",
    "  result = result.replace('<end>','')\n",
    "  # remove the <start> and <end> tokens from the sentence string\n",
    "  sentence = sentence.replace('<start> ', '')\n",
    "  sentence = sentence.replace('<end>', '')\n",
    "  \n",
    "\n",
    "  \n",
    " \n",
    "  \n",
    "  # return the result string and the original sentence\n",
    "  return sentence, result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "ancd6j2wB1GE"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('how are you today', 'i m doing great what about you')"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chatbot(\"how are you today\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "ckMrRjJ2B5sf"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('what is the weather outside', 'it s the force that pulls everything down')"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chatbot('what is the weather outside')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "aWdZjde4B8IA"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('can you run', 'what s the point')"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chatbot('can you run')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('where are you going to school', 'i m going to pcc')"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chatbot('where are you going to school')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (509022788.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[32], line 1\u001b[1;36m\u001b[0m\n\u001b[1;33m    how about a movie\u001b[0m\n\u001b[1;37m        ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "how about a movie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "how are you doing today"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chatbot('how are you doing today')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chatbot('what is your name')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chatbot('what school do you go to')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chatbot('fuck you')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chatbot('how are you')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chatbot('what about temprature')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chatbot('what about tempreature')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chatbot('what about the weather')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chatbot('what about the weather')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chatbot('whout the at abweather')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('whout the at abweather', 'of course not')"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chatbot('whout the at abweather')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('what about the weather', 'i was a painter')"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chatbot('what about the weather')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('how are you', 'fine and you')"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chatbot('how are you')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('what is your name',\n",
       " 'an established system of political administration by which a nation state district etc is governed')"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chatbot('what is your name')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('what about football',\n",
       " 'i am was never really born and therefore am effectively deathless')"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chatbot('what about football')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('how are you', 'fine and you')"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chatbot('how are you')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('what is your name',\n",
       " 'an established system of political administration by which a nation state district etc is governed')"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chatbot('what is your name')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('what school do you go to', 'i go to pcc')"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chatbot('what school do you go to')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('what about the weather ', 'i was a painter')"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chatbot('what about the weather ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('can you run', 'what s the point')"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chatbot('can you run')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
